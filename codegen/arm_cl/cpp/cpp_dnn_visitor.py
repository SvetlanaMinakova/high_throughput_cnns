from models.dnn_model.dnn import DNN
from codegen.codegen_visitor import CodegenVisitor
from codegen.arm_cl.cpp.cpp_layer_visitor import visit_layer
from codegen.arm_cl.dnn_to_streams import DNNSubStreamsGenerator


def visit_dnn(dnn: DNN, directory, profile, sub_streams_generator: DNNSubStreamsGenerator):
    """Call. code (.cpp) visitor"""
    filepath = directory + "/" + dnn.name + ".cpp"
    with open(filepath, "w") as print_file:
        visitor = DNNARMCLCPPVisitor(dnn, print_file, profile, sub_streams_generator)
        visitor.visit()


class DNNARMCLCPPVisitor(CodegenVisitor):
    def __init__(self, dnn: DNN, print_file, profile: bool, sub_streams_generator: DNNSubStreamsGenerator):
        """
        Create new CPP-code visitor of a DNN/DNN partition
        :param dnn: DNN to visit
        :param print_file: open file to print CPP code of the DNN
        :param profile: include profiling code
        """
        super().__init__(print_file, prefix="")
        self.dnn = dnn
        self.gpu_profile = profile
        self.input_layer = self.dnn.get_input_layer()
        self.output_layer = self.dnn.get_output_layer()
        self.class_name = dnn.name
        self.base_class_name = "Example"

        self.streams_generator = sub_streams_generator
        # parallel branches (streams) in the DNN
        self.sub_streams = self.streams_generator.get_sub_streams()

    def visit(self):
        self._write_common_beginning()
        self.write_line("")
        self._write_dnn()
        self.write_line("")
        self._write_common_end()

    def _write_common_beginning(self):
        self.write_line("// File automatically generated by ESPAM")
        self._include_headers()
        self.write_line("")
        self._include_namespaces()

    def _include_headers(self):
        # ARM CL classes
        arm_cl_headers = ["arm_compute/graph", "support/ToolchainSupport",
                          "utils/CommonGraphOptions", "utils/GraphUtils", "utils/Utils"]
        std_lib_headers = ["chrono", "thread"]
        local_headers = [self.class_name, "fifo", "types"]
        for header in arm_cl_headers:
            self._include_local_cpp_header(header)
        for header in std_lib_headers:
            self._include_std_cpp_header(header)
        for header in local_headers:
            self._include_local_cpp_header(header)

    def _include_namespaces(self):
        namespaces = ["arm_compute::utils", "arm_compute::graph::frontend", "arm_compute::graph_utils"]
        for namespace in namespaces:
            self._include_namespace(namespace)

    def _write_dnn(self):
        self._write_common_dnn_start()
        self._define_inputs()
        self._write_dnn_topology()
        self._define_outputs()
        self._write_common_dnn_end()

    def _define_inputs(self):
        """ Define ARM-CL (sub)-DNN input_examples(s)"""
        self.write_line("//DEFINE INPUT DATA AND START GRAPH")
        self.write_line("// Create input_examples tensor")
        inputs = self.dnn.get_inputs()
        inputs_num = len(inputs)
        # dnn should have at least 1 input_examples
        inputs_num = max(inputs_num, 1)
        if inputs_num == 1:
            self._define_single_input()
        else:
            self._define_multiple_inputs(inputs)

    def _define_single_input(self):
        """ Define single DNN input_examples"""
        # Std lines from ARM-CL examples
        self.write_line("const TensorShape tensor_shape = permute_shape(TensorShape(" +
                        "this->INPUT_W, this->INPUT_H, this->INPUT_C, 1U),"
                        " DataLayout::NCHW, common_params.data_layout);")
        self.write_line("TensorDescriptor input_descriptor = " +
                        "TensorDescriptor(tensor_shape, common_params.data_type)."
                        "set_layout(common_params.data_layout);")
        self.write_line("")

        self.write_line("graph << common_params.target")
        self.prefix_inc()
        self.prefix_inc()
        self.prefix_inc()
        self.write_line("<< common_params.fast_math_hint")
        self.write_line("<< InputLayer(input_descriptor, get_input_accessor(common_params, nullptr))")

    def _define_multiple_inputs(self, inputs):
        """ Define multiple DNN inputs"""
        # Std lines from ARM-CL examples
        # define input_examples tensors
        for dnn_input in inputs:
            data_layer = dnn_input.data_layer
            input_name = data_layer.name
            self.write_line("const TensorShape " + input_name + "_shape = permute_shape(TensorShape(" +
                            str(int(data_layer.ow)) + "U, " +
                            str(int(data_layer.oh)) + "U, " +
                            str(int(data_layer.ofm)) + "U, " +
                            "1U)," +
                            " DataLayout::NCHW, common_params.data_layout);")
            self.write_line("TensorDescriptor " + input_name + "_descriptor = " +
                            "TensorDescriptor(" + input_name + "_shape, common_params.data_type)."
                                                               "set_layout(common_params.data_layout);")
            self.write_line("")

        # define graph
        self.write_line("graph << common_params.target")
        self.prefix_inc()
        self.prefix_inc()
        self.prefix_inc()
        self.write_line("<< common_params.fast_math_hint;")

        # define inputs in sub-branches
        sub_stream_in_group_id = 0
        for dnn_input in inputs:
            input_name = dnn_input.data_layer.name
            self.write_line("//sub_stream" + str(sub_stream_in_group_id))
            self.write_line("SubStream sub_stream" + str(sub_stream_in_group_id) + "(graph);")
            self.write_line("sub_stream" + str(sub_stream_in_group_id) +
                            "<< InputLayer(" + input_name + "_descriptor, get_input_accessor(common_params, nullptr));")
            self.write_line("")
            sub_stream_in_group_id += 1

    def _write_dnn_topology(self):
        """create DNN topology"""
        if self.streams_generator.is_multi_branch_dnn():
            self._visit_multi_branch_dnn()
        else:
            self._visit_linear_dnn()

    def _visit_linear_dnn(self):
        """ Visit DNN which has no branches: every layer accepts one input_examples and produces one output"""
        for layer in self.dnn.get_layers():
            visit_layer(self.dnn, layer, self.print_file, self.prefix)
            # self.write_line("")

    def _visit_multi_branch_dnn(self):
        """ Visit DNN which has branches: layers can accept/produce data from/to multiple other layers"""
        sub_stream_groups = self.streams_generator.get_sub_stream_groups()
        for group_id in range(len(sub_stream_groups)):
            group = sub_stream_groups[group_id]
            sub_streams = group.sub_streams
            if (len(sub_streams)) > 1:
                self._define_sub_stream_group_as_function_call(group_id)
            else:
                single_sub_stream = sub_streams[0]
                sub_stream_inputs = group.inputs
                if len(sub_stream_inputs) > 1:
                    # skip definition, because it belongs to one of the sub-function calls
                    pass
                else:
                    self._define_sub_stream_in_main_graph(single_sub_stream, group_id)

    def _define_sub_stream_in_main_graph(self, sub_stream, group_id):
        if group_id > 0:
            self.write(self.prefix + "graph ")
        layer_in_sub_stream_id = 0
        for layer_name in sub_stream:
            layer = self.dnn.find_layer_by_name(layer_name)
            end_sub_stream = self.__is_substream_end(layer_in_sub_stream_id, len(sub_stream), group_id)
            visit_layer(self.dnn, layer, self.print_file, self.prefix, end_substream=end_sub_stream)
            layer_in_sub_stream_id += 1
        if group_id == 0:
            self.write_line("")

    def __is_substream_end(self, layer_in_sub_stream_id, sub_stream_len, group_id):
        # last group ends with last layer, defined separately
        if group_id == len(self.streams_generator.get_sub_stream_groups())-1:
            return False

        if layer_in_sub_stream_id == sub_stream_len - 1:
            return True

        return False

    def _define_sub_stream_group_as_function_call(self, group_id):
        self.prefix_dec()
        self.write_line("add_sub_streams_group_" + str(group_id) + "(data_path, weights_layout);")
        self.write_line("")
        self.prefix_inc()

    def _define_outputs(self):
        """
        Std line from ARM-CL examples
        "5" is always specified for the out[ut layer by ARM-CL
        """
        self.write_line("<< OutputLayer(get_output_accessor(common_params, 5));")

    def _write_common_dnn_start(self):
        self.write_line("//NETWORK ENGINE WITH API")
        self.write_line("bool " + self.class_name + "::do_setup(int argc, char **argv) {")
        self.prefix_inc()

        self.write_line("")
        self.write_line("// Parse arguments")
        self.write_line("cmd_parser.parse(argc, argv);")
        self.write_line("cmd_parser.validate();")
        self.write_line("")

        self.write_line("// Consume common parameters")
        self.write_line("common_params = consume_common_graph_parameters(common_opts);")
        self.write_line("")

        self.write_line("// Return when help menu is requested")
        self.write_line("if(common_params.help){")
        self.prefix_inc()
        self.write_line("cmd_parser.print_help(argv[0]);")
        self.write_line("return false;")
        self.prefix_dec()
        self.write_line("}")
        self.write_line("")

        self.write_line("// Checks")
        self.write_line("ARM_COMPUTE_EXIT_ON_MSG(arm_compute::is_data_type_quantized_asymmetric(common_params.data_type), " +
                        "\"QASYMM8 not supported for this graph\");")

        self.write_line("// Get trainable parameters data path")
        self.write_line("std::string data_path = common_params.data_path;")
        self.write_line("")

        self.write_line("// Set weights trained layout")
        self.write_line("const DataLayout weights_layout = DataLayout::NCHW;")
        self.write_line("")

    def _write_common_dnn_end(self):
        self.prefix_dec()
        self.prefix_dec()
        self.prefix_dec()

        self.write_line("")
        self.write_line("")

        self.write_line("// Finalize graph")
        self.write_line("GraphConfig config;")
        self.write_line("")

        self.write_line("config.num_threads = common_params.threads;")
        self.write_line("config.use_tuner   = common_params.enable_tuner;")
        self.write_line("config.tuner_mode  = common_params.tuner_mode;")
        self.write_line("config.tuner_file  = common_params.tuner_file;")

        self.write_line("")
        self.write_line("// Load the precompiled kernels from a file into the kernel library, " +
                        "in this way the next time they are needed")
        self.write_line("// compilation won't be required.")
        self.write_line("if(common_params.enable_cl_cache) {")
        self.prefix_inc()
        
        self.write_line("restore_program_cache_from_file();")
        self.prefix_dec()
        self.write_line("}")
        self.write_line("")

        self.write_line("graph.finalize(common_params.target, config);")
        self.write_line("")

        self.write_line("// Save the opencl kernels to a file")
        self.write_line("if(common_opts.enable_cl_cache) { ")
        self.prefix_inc()
        self.write_line(" save_program_cache_to_file();")
        self.prefix_dec()
        self.write_line("}")
        self.write_line("")

        self.write_line("return true;")
        self.prefix_dec()

        self.write_line("}")

    def _write_common_end(self):
        self.write_do_run()
        self._write_parallel_branches()

    def write_do_run(self):
        self.write_line("// Run graph")
        self.write_line("void " + self.class_name + "::do_run() { ")
        self.prefix_inc()
        self.write_line("graph.run();")
        self.prefix_dec()
        self.write_line("}")
        self.write_line("")

    def _write_parallel_branches(self):
        if not self.streams_generator.is_multi_branch_dnn():
            return

        sub_stream_groups = self.streams_generator.get_sub_stream_groups()
        for group_id in range(len(sub_stream_groups)):
            group = sub_stream_groups[group_id]
            sub_streams = group.sub_streams
            if (len(sub_streams)) > 1:
                self._define_function_call_for_substream_group(group, group_id)

    def _define_function_call_for_substream_group(self, group, group_id):
        total_groups = len(self.streams_generator.get_sub_stream_groups())
        # "void " + self.class_name + "::do_run() { ")
        self.write_line("void " + self.class_name +
                        "::add_sub_streams_group_" + str(group_id) +
                        "(const std::string &data_path, DataLayout weights_layout) {")
        self.prefix_inc()
        self.write_line("")

        for sub_stream_in_group_id in range(len(group.sub_streams)):
            self.write_line("//sub_stream" + str(sub_stream_in_group_id))
            self.write_line("SubStream sub_stream" + str(sub_stream_in_group_id) + "(graph);")
            sub_stream = group.sub_streams[sub_stream_in_group_id]
            layer_in_sub_stream_id = 0
            for layer_name in sub_stream:
                if layer_in_sub_stream_id == 0:
                    self.write(self.prefix + "sub_stream" + str(sub_stream_in_group_id))
                layer = self.dnn.find_layer_by_name(layer_name)

                end_sub_stream = self.__is_substream_end(layer_in_sub_stream_id, len(sub_stream), group_id)

                visit_layer(self.dnn, layer, self.print_file, self.prefix, end_substream=end_sub_stream)
                layer_in_sub_stream_id += 1
                if end_sub_stream:
                    self.write_line("")

        try:
            group_outputs = group.outputs
            merger_sub_stream_id = group_outputs[0]
            merger_sub_stream = self.streams_generator.get_sub_streams()[merger_sub_stream_id]
            merger_name = merger_sub_stream[0]
            merger_layer = self.dnn.find_layer_by_name(merger_name)

            visit_layer(self.dnn, merger_layer, self.print_file, self.prefix, end_substream=True)

        except Exception:
            self.write_line("//ERROR: I could not generate merger (add/concat/etc) for streams group")

        self.prefix_dec()
        self.write_line("}")
        self.write_line("")




"""
private:
    CommandLineParser           cmd_parser;
    CommonGraphOptions          common_opts;
    SimpleOption<unsigned int> *model_input_width{ nullptr };
    SimpleOption<unsigned int> *model_input_height{ nullptr };
    CommonGraphParams           common_params;
    Stream                      graph;

    void add_residual_block(const std::string &data_path, const std::string &name, DataLayout weights_layout)
    {
"""
        
